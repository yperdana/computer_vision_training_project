{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First Computer Vision Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxTIh7N8GqN/TWwSYh2ZR9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yperdana/computer_vision_training_project/blob/master/First_Computer_Vision_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok9Z_redfNti",
        "colab_type": "text"
      },
      "source": [
        "This Project try to recognize different items of clothings.\n",
        "Data from  MNIST and we try to optimize using neural network approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeaU7VqkfOlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26d11466-5cde-4be6-d891-e2a0cbb3de02"
      },
      "source": [
        "#import tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhljvDr3fqd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data\n",
        "mnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEwf0DuLfvIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "eb566e32-6af0-4d88-8ebb-3722dfd024ed"
      },
      "source": [
        "# Separating training and test dataset\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi1t-Ug4f1aG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import other libraries\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=200)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gORU5lX4f-dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#do nomalization\n",
        "\n",
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6cIrJ69gDMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#design model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBfmrnKMgKog",
        "colab_type": "text"
      },
      "source": [
        "Flatten: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
        "\n",
        "Dense: Adds a layer of neurons\n",
        "\n",
        "Each layer of neurons need an activation function to tell them what to do. There's lots of options, but just use these for now.\n",
        "\n",
        "Relu effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.\n",
        "\n",
        "Softmax takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mVUTjrxgHLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98648d76-bd38-4f61-c493-74e5e0d7cd87"
      },
      "source": [
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=30)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2793 - accuracy: 0.8958\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2669 - accuracy: 0.9011\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2552 - accuracy: 0.9050\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2473 - accuracy: 0.9071\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2368 - accuracy: 0.9105\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2309 - accuracy: 0.9121\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2229 - accuracy: 0.9157\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2159 - accuracy: 0.9185\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2101 - accuracy: 0.9210\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2027 - accuracy: 0.9233\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1964 - accuracy: 0.9253\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1923 - accuracy: 0.9281\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1879 - accuracy: 0.9301\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1833 - accuracy: 0.9299\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1782 - accuracy: 0.9325\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1728 - accuracy: 0.9356\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1692 - accuracy: 0.9360\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1642 - accuracy: 0.9379\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1612 - accuracy: 0.9398\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1566 - accuracy: 0.9407\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1535 - accuracy: 0.9428\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1499 - accuracy: 0.9439\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1484 - accuracy: 0.9441\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1446 - accuracy: 0.9456\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1415 - accuracy: 0.9465\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1377 - accuracy: 0.9483\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1355 - accuracy: 0.9492\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1345 - accuracy: 0.9493\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1285 - accuracy: 0.9520\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1270 - accuracy: 0.9521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4134a27278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}